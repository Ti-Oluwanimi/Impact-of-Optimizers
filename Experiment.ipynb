{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. IMPORTANT LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toluwani/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. LOADING & USING PYTORCH TRANSFORMS ON CIFAR10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#augmentations and transforms to be used on the train and test sets\n",
    "transform = transforms.Compose([\n",
    "                                transforms.RandomHorizontalFlip(0.5),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "\n",
    "#applying the transforms to the train-test set\n",
    "train_ds = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_ds = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "#creating the train and test loaders from their respective sets\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. CODING THE LENET-5 AND ALEXNET ARCHITECTURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Sequential):\n",
    "    def __init__(self, img_channels=3, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(img_channels, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, img_channels=3, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=img_channels, out_channels= 96, kernel_size= 3, stride=1, padding=0 )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=3, stride= 1, padding= 1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride= 1, padding= 1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1  = nn.Linear(in_features= 9216, out_features= 4096)\n",
    "        self.fc2  = nn.Linear(in_features= 4096, out_features= 4096)\n",
    "        self.fc3 = nn.Linear(in_features=4096 , out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. SETTING UP GPU AND CRITERION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #to train on GPU, else CPU\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. DEFINING THE TRAIN AND TEST FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, train_loader, test_loader, optimizer, n_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #to train on GPU, else CPU\n",
    "    # model in training mode\n",
    "    model.train()\n",
    "    train_l = []\n",
    "    test_acc = []\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_accuracy = 0\n",
    "        train_samples = 0\n",
    "        train_loss = 0.0\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            predictions = torch.argmax(output, dim=-1)\n",
    "            train_samples += predictions.size(0)\n",
    "            train_accuracy += (predictions == targets).sum()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_l.append(train_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_accuracy = 0\n",
    "            test_samples = 0\n",
    "            for data,targets in test_loader:\n",
    "                data = data.to(device=device)\n",
    "                targets = targets.to(device=device)\n",
    "                ## Forward Pass\n",
    "                scores = model(data)\n",
    "                loss = criterion(scores,targets)\n",
    "                predictions = torch.argmax(scores, dim=-1)\n",
    "                test_accuracy += (predictions == targets).sum()\n",
    "                test_samples += predictions.size(0)\n",
    "                test_loss += loss.item() \n",
    "            t_a = (test_accuracy / test_samples)*100\n",
    "            test_acc.append(t_a)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss), f\"  Test Accuracy: {t_a:.3f}\")\n",
    "            \n",
    "    return model, train_l, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5(A). TRAINING LENET WITH SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.304535   Test Accuracy: 10.000\n",
      "Epoch: 2 \tTraining Loss: 2.302970   Test Accuracy: 10.020\n",
      "Epoch: 3 \tTraining Loss: 2.301555   Test Accuracy: 11.170\n",
      "Epoch: 4 \tTraining Loss: 2.300057   Test Accuracy: 14.420\n",
      "Epoch: 5 \tTraining Loss: 2.297940   Test Accuracy: 16.050\n",
      "Epoch: 6 \tTraining Loss: 2.294215   Test Accuracy: 17.070\n",
      "Epoch: 7 \tTraining Loss: 2.285947   Test Accuracy: 19.070\n",
      "Epoch: 8 \tTraining Loss: 2.262126   Test Accuracy: 19.480\n",
      "Epoch: 9 \tTraining Loss: 2.181979   Test Accuracy: 21.770\n",
      "Epoch: 10 \tTraining Loss: 2.060597   Test Accuracy: 26.220\n",
      "Epoch: 11 \tTraining Loss: 1.983677   Test Accuracy: 28.370\n",
      "Epoch: 12 \tTraining Loss: 1.934992   Test Accuracy: 30.560\n",
      "Epoch: 13 \tTraining Loss: 1.888738   Test Accuracy: 32.110\n",
      "Epoch: 14 \tTraining Loss: 1.846068   Test Accuracy: 33.840\n",
      "Epoch: 15 \tTraining Loss: 1.803572   Test Accuracy: 35.100\n",
      "Epoch: 16 \tTraining Loss: 1.762213   Test Accuracy: 36.380\n",
      "Epoch: 17 \tTraining Loss: 1.722939   Test Accuracy: 37.580\n",
      "Epoch: 18 \tTraining Loss: 1.689037   Test Accuracy: 38.440\n",
      "Epoch: 19 \tTraining Loss: 1.658733   Test Accuracy: 39.540\n",
      "Epoch: 20 \tTraining Loss: 1.633645   Test Accuracy: 40.440\n",
      "Epoch: 21 \tTraining Loss: 1.610836   Test Accuracy: 41.280\n",
      "Epoch: 22 \tTraining Loss: 1.590090   Test Accuracy: 42.100\n",
      "Epoch: 23 \tTraining Loss: 1.569225   Test Accuracy: 42.280\n",
      "Epoch: 24 \tTraining Loss: 1.551410   Test Accuracy: 43.790\n",
      "Epoch: 25 \tTraining Loss: 1.533812   Test Accuracy: 44.250\n",
      "Epoch: 26 \tTraining Loss: 1.517326   Test Accuracy: 45.000\n",
      "Epoch: 27 \tTraining Loss: 1.503145   Test Accuracy: 45.560\n",
      "Epoch: 28 \tTraining Loss: 1.487787   Test Accuracy: 46.810\n",
      "Epoch: 29 \tTraining Loss: 1.475107   Test Accuracy: 46.600\n",
      "Epoch: 30 \tTraining Loss: 1.461125   Test Accuracy: 47.240\n",
      "Epoch: 31 \tTraining Loss: 1.447758   Test Accuracy: 47.220\n",
      "Epoch: 32 \tTraining Loss: 1.436992   Test Accuracy: 47.760\n",
      "Epoch: 33 \tTraining Loss: 1.422715   Test Accuracy: 48.390\n",
      "Epoch: 34 \tTraining Loss: 1.412815   Test Accuracy: 48.380\n",
      "Epoch: 35 \tTraining Loss: 1.402059   Test Accuracy: 49.800\n",
      "Epoch: 36 \tTraining Loss: 1.389646   Test Accuracy: 50.240\n",
      "Epoch: 37 \tTraining Loss: 1.378053   Test Accuracy: 50.380\n",
      "Epoch: 38 \tTraining Loss: 1.366277   Test Accuracy: 50.040\n",
      "Epoch: 39 \tTraining Loss: 1.355353   Test Accuracy: 51.140\n",
      "Epoch: 40 \tTraining Loss: 1.347194   Test Accuracy: 51.750\n",
      "Epoch: 41 \tTraining Loss: 1.335044   Test Accuracy: 51.180\n",
      "Epoch: 42 \tTraining Loss: 1.324588   Test Accuracy: 51.750\n",
      "Epoch: 43 \tTraining Loss: 1.313912   Test Accuracy: 52.290\n",
      "Epoch: 44 \tTraining Loss: 1.305827   Test Accuracy: 52.700\n",
      "Epoch: 45 \tTraining Loss: 1.297776   Test Accuracy: 53.380\n",
      "Epoch: 46 \tTraining Loss: 1.287022   Test Accuracy: 53.090\n",
      "Epoch: 47 \tTraining Loss: 1.280458   Test Accuracy: 53.740\n",
      "Epoch: 48 \tTraining Loss: 1.274214   Test Accuracy: 53.380\n",
      "Epoch: 49 \tTraining Loss: 1.263774   Test Accuracy: 54.440\n",
      "Epoch: 50 \tTraining Loss: 1.259449   Test Accuracy: 54.030\n"
     ]
    }
   ],
   "source": [
    "lenet = LeNet().to(device)\n",
    "sgd_lenet = optim.SGD(lenet.parameters(), lr=1e-3) #SGD\n",
    "lenet_sgd, loss_sgd, acc_sgd = train_test(lenet, train_dl, test_dl, sgd_lenet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5(B). TRAINING LENET WITH SGD+MOMENTUM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.235306   Test Accuracy: 27.830\n",
      "Epoch: 2 \tTraining Loss: 1.764317   Test Accuracy: 41.280\n",
      "Epoch: 3 \tTraining Loss: 1.556811   Test Accuracy: 46.090\n",
      "Epoch: 4 \tTraining Loss: 1.462720   Test Accuracy: 49.220\n",
      "Epoch: 5 \tTraining Loss: 1.384669   Test Accuracy: 51.230\n",
      "Epoch: 6 \tTraining Loss: 1.326268   Test Accuracy: 52.880\n",
      "Epoch: 7 \tTraining Loss: 1.268522   Test Accuracy: 55.210\n",
      "Epoch: 8 \tTraining Loss: 1.225106   Test Accuracy: 56.800\n",
      "Epoch: 9 \tTraining Loss: 1.189702   Test Accuracy: 57.970\n",
      "Epoch: 10 \tTraining Loss: 1.155510   Test Accuracy: 58.770\n",
      "Epoch: 11 \tTraining Loss: 1.124351   Test Accuracy: 60.150\n",
      "Epoch: 12 \tTraining Loss: 1.093768   Test Accuracy: 60.430\n",
      "Epoch: 13 \tTraining Loss: 1.071486   Test Accuracy: 60.830\n",
      "Epoch: 14 \tTraining Loss: 1.047049   Test Accuracy: 60.440\n",
      "Epoch: 15 \tTraining Loss: 1.028445   Test Accuracy: 62.140\n",
      "Epoch: 16 \tTraining Loss: 1.004781   Test Accuracy: 63.620\n",
      "Epoch: 17 \tTraining Loss: 0.983164   Test Accuracy: 64.190\n",
      "Epoch: 18 \tTraining Loss: 0.964141   Test Accuracy: 64.380\n",
      "Epoch: 19 \tTraining Loss: 0.947488   Test Accuracy: 65.290\n",
      "Epoch: 20 \tTraining Loss: 0.928091   Test Accuracy: 65.360\n",
      "Epoch: 21 \tTraining Loss: 0.915461   Test Accuracy: 65.390\n",
      "Epoch: 22 \tTraining Loss: 0.896368   Test Accuracy: 65.910\n",
      "Epoch: 23 \tTraining Loss: 0.878508   Test Accuracy: 66.330\n",
      "Epoch: 24 \tTraining Loss: 0.866491   Test Accuracy: 66.280\n",
      "Epoch: 25 \tTraining Loss: 0.856003   Test Accuracy: 67.550\n",
      "Epoch: 26 \tTraining Loss: 0.839382   Test Accuracy: 67.080\n",
      "Epoch: 27 \tTraining Loss: 0.827682   Test Accuracy: 67.220\n",
      "Epoch: 28 \tTraining Loss: 0.815203   Test Accuracy: 67.380\n",
      "Epoch: 29 \tTraining Loss: 0.806568   Test Accuracy: 66.120\n",
      "Epoch: 30 \tTraining Loss: 0.796864   Test Accuracy: 67.770\n",
      "Epoch: 31 \tTraining Loss: 0.781004   Test Accuracy: 67.790\n",
      "Epoch: 32 \tTraining Loss: 0.774723   Test Accuracy: 67.320\n",
      "Epoch: 33 \tTraining Loss: 0.766922   Test Accuracy: 67.670\n",
      "Epoch: 34 \tTraining Loss: 0.756771   Test Accuracy: 67.970\n",
      "Epoch: 35 \tTraining Loss: 0.747929   Test Accuracy: 68.070\n",
      "Epoch: 36 \tTraining Loss: 0.741913   Test Accuracy: 68.310\n",
      "Epoch: 37 \tTraining Loss: 0.731159   Test Accuracy: 67.590\n",
      "Epoch: 38 \tTraining Loss: 0.719044   Test Accuracy: 67.200\n",
      "Epoch: 39 \tTraining Loss: 0.717379   Test Accuracy: 68.050\n",
      "Epoch: 40 \tTraining Loss: 0.709462   Test Accuracy: 68.380\n",
      "Epoch: 41 \tTraining Loss: 0.694703   Test Accuracy: 68.630\n",
      "Epoch: 42 \tTraining Loss: 0.691344   Test Accuracy: 68.390\n",
      "Epoch: 43 \tTraining Loss: 0.682124   Test Accuracy: 67.600\n",
      "Epoch: 44 \tTraining Loss: 0.677807   Test Accuracy: 68.000\n",
      "Epoch: 45 \tTraining Loss: 0.671780   Test Accuracy: 68.200\n",
      "Epoch: 46 \tTraining Loss: 0.664310   Test Accuracy: 68.490\n",
      "Epoch: 47 \tTraining Loss: 0.659748   Test Accuracy: 68.490\n",
      "Epoch: 48 \tTraining Loss: 0.652512   Test Accuracy: 69.160\n",
      "Epoch: 49 \tTraining Loss: 0.646025   Test Accuracy: 68.270\n",
      "Epoch: 50 \tTraining Loss: 0.634893   Test Accuracy: 68.490\n"
     ]
    }
   ],
   "source": [
    "lenet = LeNet().to(device)\n",
    "sgdm_lenet = optim.SGD(lenet.parameters(), lr=1e-3, momentum=0.9) #SGD with momentum\n",
    "lenet_sgdm, loss_sgdm, acc_sgdm = train_test(lenet, train_dl, test_dl, sgdm_lenet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5(C). TRAINING LENET WITH ADAGRAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.000127   Test Accuracy: 31.170\n",
      "Epoch: 2 \tTraining Loss: 1.891782   Test Accuracy: 33.340\n",
      "Epoch: 3 \tTraining Loss: 1.845419   Test Accuracy: 34.370\n",
      "Epoch: 4 \tTraining Loss: 1.810862   Test Accuracy: 35.480\n",
      "Epoch: 5 \tTraining Loss: 1.783588   Test Accuracy: 36.640\n",
      "Epoch: 6 \tTraining Loss: 1.758252   Test Accuracy: 36.840\n",
      "Epoch: 7 \tTraining Loss: 1.735358   Test Accuracy: 38.040\n",
      "Epoch: 8 \tTraining Loss: 1.713322   Test Accuracy: 38.710\n",
      "Epoch: 9 \tTraining Loss: 1.692633   Test Accuracy: 39.650\n",
      "Epoch: 10 \tTraining Loss: 1.673893   Test Accuracy: 40.060\n",
      "Epoch: 11 \tTraining Loss: 1.656547   Test Accuracy: 40.820\n",
      "Epoch: 12 \tTraining Loss: 1.640804   Test Accuracy: 41.070\n",
      "Epoch: 13 \tTraining Loss: 1.628029   Test Accuracy: 41.470\n",
      "Epoch: 14 \tTraining Loss: 1.615621   Test Accuracy: 41.730\n",
      "Epoch: 15 \tTraining Loss: 1.604297   Test Accuracy: 42.420\n",
      "Epoch: 16 \tTraining Loss: 1.594113   Test Accuracy: 42.420\n",
      "Epoch: 17 \tTraining Loss: 1.584729   Test Accuracy: 42.690\n",
      "Epoch: 18 \tTraining Loss: 1.576375   Test Accuracy: 42.770\n",
      "Epoch: 19 \tTraining Loss: 1.568488   Test Accuracy: 43.440\n",
      "Epoch: 20 \tTraining Loss: 1.561946   Test Accuracy: 43.570\n",
      "Epoch: 21 \tTraining Loss: 1.555184   Test Accuracy: 43.830\n",
      "Epoch: 22 \tTraining Loss: 1.548608   Test Accuracy: 43.930\n",
      "Epoch: 23 \tTraining Loss: 1.542586   Test Accuracy: 44.040\n",
      "Epoch: 24 \tTraining Loss: 1.536848   Test Accuracy: 44.470\n",
      "Epoch: 25 \tTraining Loss: 1.531445   Test Accuracy: 44.620\n",
      "Epoch: 26 \tTraining Loss: 1.526032   Test Accuracy: 44.730\n",
      "Epoch: 27 \tTraining Loss: 1.522900   Test Accuracy: 45.160\n",
      "Epoch: 28 \tTraining Loss: 1.518260   Test Accuracy: 44.720\n",
      "Epoch: 29 \tTraining Loss: 1.513104   Test Accuracy: 45.350\n",
      "Epoch: 30 \tTraining Loss: 1.509488   Test Accuracy: 45.740\n",
      "Epoch: 31 \tTraining Loss: 1.506193   Test Accuracy: 45.630\n",
      "Epoch: 32 \tTraining Loss: 1.501488   Test Accuracy: 45.610\n",
      "Epoch: 33 \tTraining Loss: 1.497706   Test Accuracy: 46.060\n",
      "Epoch: 34 \tTraining Loss: 1.494284   Test Accuracy: 45.820\n",
      "Epoch: 35 \tTraining Loss: 1.490252   Test Accuracy: 46.070\n",
      "Epoch: 36 \tTraining Loss: 1.487187   Test Accuracy: 46.150\n",
      "Epoch: 37 \tTraining Loss: 1.484474   Test Accuracy: 46.110\n",
      "Epoch: 38 \tTraining Loss: 1.480729   Test Accuracy: 46.900\n",
      "Epoch: 39 \tTraining Loss: 1.477224   Test Accuracy: 46.500\n",
      "Epoch: 40 \tTraining Loss: 1.474460   Test Accuracy: 47.040\n",
      "Epoch: 41 \tTraining Loss: 1.470965   Test Accuracy: 46.960\n",
      "Epoch: 42 \tTraining Loss: 1.468374   Test Accuracy: 47.350\n",
      "Epoch: 43 \tTraining Loss: 1.466213   Test Accuracy: 47.620\n",
      "Epoch: 44 \tTraining Loss: 1.463534   Test Accuracy: 47.230\n",
      "Epoch: 45 \tTraining Loss: 1.459616   Test Accuracy: 47.450\n",
      "Epoch: 46 \tTraining Loss: 1.457438   Test Accuracy: 47.440\n",
      "Epoch: 47 \tTraining Loss: 1.456243   Test Accuracy: 47.820\n",
      "Epoch: 48 \tTraining Loss: 1.453477   Test Accuracy: 47.560\n",
      "Epoch: 49 \tTraining Loss: 1.451896   Test Accuracy: 47.940\n",
      "Epoch: 50 \tTraining Loss: 1.448655   Test Accuracy: 48.080\n"
     ]
    }
   ],
   "source": [
    "lenet = LeNet().to(device)\n",
    "ag_lenet = optim.Adagrad(lenet.parameters(), lr=1e-3) #Adagrad\n",
    "lenet_ag, loss_ag, acc_ag = train_test(lenet, train_dl, test_dl, ag_lenet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5(D). TRAINING LENET WITH RMSPROP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.597867   Test Accuracy: 46.500\n",
      "Epoch: 2 \tTraining Loss: 1.340487   Test Accuracy: 53.890\n",
      "Epoch: 3 \tTraining Loss: 1.222722   Test Accuracy: 51.720\n",
      "Epoch: 4 \tTraining Loss: 1.141108   Test Accuracy: 60.780\n",
      "Epoch: 5 \tTraining Loss: 1.077743   Test Accuracy: 61.250\n",
      "Epoch: 6 \tTraining Loss: 1.026840   Test Accuracy: 63.040\n",
      "Epoch: 7 \tTraining Loss: 0.996906   Test Accuracy: 61.680\n",
      "Epoch: 8 \tTraining Loss: 0.963120   Test Accuracy: 63.790\n",
      "Epoch: 9 \tTraining Loss: 0.938219   Test Accuracy: 64.810\n",
      "Epoch: 10 \tTraining Loss: 0.914719   Test Accuracy: 63.500\n",
      "Epoch: 11 \tTraining Loss: 0.896194   Test Accuracy: 66.210\n",
      "Epoch: 12 \tTraining Loss: 0.875843   Test Accuracy: 63.470\n",
      "Epoch: 13 \tTraining Loss: 0.858988   Test Accuracy: 65.510\n",
      "Epoch: 14 \tTraining Loss: 0.848570   Test Accuracy: 64.590\n",
      "Epoch: 15 \tTraining Loss: 0.838506   Test Accuracy: 66.310\n",
      "Epoch: 16 \tTraining Loss: 0.818406   Test Accuracy: 66.420\n",
      "Epoch: 17 \tTraining Loss: 0.805991   Test Accuracy: 66.080\n",
      "Epoch: 18 \tTraining Loss: 0.800964   Test Accuracy: 66.610\n",
      "Epoch: 19 \tTraining Loss: 0.786431   Test Accuracy: 66.890\n",
      "Epoch: 20 \tTraining Loss: 0.776097   Test Accuracy: 66.110\n",
      "Epoch: 21 \tTraining Loss: 0.768365   Test Accuracy: 66.830\n",
      "Epoch: 22 \tTraining Loss: 0.764415   Test Accuracy: 67.120\n",
      "Epoch: 23 \tTraining Loss: 0.753988   Test Accuracy: 65.730\n",
      "Epoch: 24 \tTraining Loss: 0.745124   Test Accuracy: 66.630\n",
      "Epoch: 25 \tTraining Loss: 0.736874   Test Accuracy: 66.600\n",
      "Epoch: 26 \tTraining Loss: 0.734193   Test Accuracy: 66.750\n",
      "Epoch: 27 \tTraining Loss: 0.727511   Test Accuracy: 65.720\n",
      "Epoch: 28 \tTraining Loss: 0.721309   Test Accuracy: 66.460\n",
      "Epoch: 29 \tTraining Loss: 0.716682   Test Accuracy: 66.830\n",
      "Epoch: 30 \tTraining Loss: 0.708173   Test Accuracy: 66.140\n",
      "Epoch: 31 \tTraining Loss: 0.703100   Test Accuracy: 66.230\n",
      "Epoch: 32 \tTraining Loss: 0.699121   Test Accuracy: 66.240\n",
      "Epoch: 33 \tTraining Loss: 0.694628   Test Accuracy: 66.670\n",
      "Epoch: 34 \tTraining Loss: 0.688345   Test Accuracy: 66.550\n",
      "Epoch: 35 \tTraining Loss: 0.684290   Test Accuracy: 67.580\n",
      "Epoch: 36 \tTraining Loss: 0.680038   Test Accuracy: 66.590\n",
      "Epoch: 37 \tTraining Loss: 0.678217   Test Accuracy: 65.830\n",
      "Epoch: 38 \tTraining Loss: 0.673259   Test Accuracy: 67.140\n",
      "Epoch: 39 \tTraining Loss: 0.667691   Test Accuracy: 67.940\n",
      "Epoch: 40 \tTraining Loss: 0.669841   Test Accuracy: 65.660\n",
      "Epoch: 41 \tTraining Loss: 0.662509   Test Accuracy: 67.310\n",
      "Epoch: 42 \tTraining Loss: 0.655476   Test Accuracy: 66.780\n",
      "Epoch: 43 \tTraining Loss: 0.658857   Test Accuracy: 67.150\n",
      "Epoch: 44 \tTraining Loss: 0.656861   Test Accuracy: 66.920\n",
      "Epoch: 45 \tTraining Loss: 0.644290   Test Accuracy: 67.330\n",
      "Epoch: 46 \tTraining Loss: 0.643350   Test Accuracy: 65.870\n",
      "Epoch: 47 \tTraining Loss: 0.643533   Test Accuracy: 66.140\n",
      "Epoch: 48 \tTraining Loss: 0.642219   Test Accuracy: 65.800\n",
      "Epoch: 49 \tTraining Loss: 0.632936   Test Accuracy: 66.840\n",
      "Epoch: 50 \tTraining Loss: 0.629818   Test Accuracy: 65.180\n"
     ]
    }
   ],
   "source": [
    "lenet = LeNet().to(device)\n",
    "rms_lenet = optim.RMSprop(lenet.parameters(), lr=1e-3) #RMSprop\n",
    "lenet_rms, loss_rms, acc_rms = train_test(lenet, train_dl, test_dl, rms_lenet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5(E). TRAINING LENET WITH ADAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.589832   Test Accuracy: 49.990\n",
      "Epoch: 2 \tTraining Loss: 1.283900   Test Accuracy: 57.130\n",
      "Epoch: 3 \tTraining Loss: 1.162282   Test Accuracy: 60.130\n",
      "Epoch: 4 \tTraining Loss: 1.081875   Test Accuracy: 61.660\n",
      "Epoch: 5 \tTraining Loss: 1.031880   Test Accuracy: 62.620\n",
      "Epoch: 6 \tTraining Loss: 0.988591   Test Accuracy: 63.250\n",
      "Epoch: 7 \tTraining Loss: 0.951032   Test Accuracy: 65.590\n",
      "Epoch: 8 \tTraining Loss: 0.928902   Test Accuracy: 64.730\n",
      "Epoch: 9 \tTraining Loss: 0.898819   Test Accuracy: 65.820\n",
      "Epoch: 10 \tTraining Loss: 0.877524   Test Accuracy: 65.690\n",
      "Epoch: 11 \tTraining Loss: 0.860367   Test Accuracy: 66.290\n",
      "Epoch: 12 \tTraining Loss: 0.849568   Test Accuracy: 65.940\n",
      "Epoch: 13 \tTraining Loss: 0.828032   Test Accuracy: 67.270\n",
      "Epoch: 14 \tTraining Loss: 0.814995   Test Accuracy: 66.750\n",
      "Epoch: 15 \tTraining Loss: 0.802464   Test Accuracy: 67.280\n",
      "Epoch: 16 \tTraining Loss: 0.794991   Test Accuracy: 66.190\n",
      "Epoch: 17 \tTraining Loss: 0.782402   Test Accuracy: 67.150\n",
      "Epoch: 18 \tTraining Loss: 0.774466   Test Accuracy: 67.170\n",
      "Epoch: 19 \tTraining Loss: 0.760128   Test Accuracy: 67.180\n",
      "Epoch: 20 \tTraining Loss: 0.755447   Test Accuracy: 67.470\n",
      "Epoch: 21 \tTraining Loss: 0.745222   Test Accuracy: 67.930\n",
      "Epoch: 22 \tTraining Loss: 0.737540   Test Accuracy: 66.670\n",
      "Epoch: 23 \tTraining Loss: 0.728904   Test Accuracy: 67.690\n",
      "Epoch: 24 \tTraining Loss: 0.729362   Test Accuracy: 67.540\n",
      "Epoch: 25 \tTraining Loss: 0.716992   Test Accuracy: 66.260\n",
      "Epoch: 26 \tTraining Loss: 0.708721   Test Accuracy: 67.980\n",
      "Epoch: 27 \tTraining Loss: 0.705574   Test Accuracy: 67.030\n",
      "Epoch: 28 \tTraining Loss: 0.698044   Test Accuracy: 66.910\n",
      "Epoch: 29 \tTraining Loss: 0.687498   Test Accuracy: 66.800\n",
      "Epoch: 30 \tTraining Loss: 0.688149   Test Accuracy: 66.570\n",
      "Epoch: 31 \tTraining Loss: 0.682025   Test Accuracy: 67.400\n",
      "Epoch: 32 \tTraining Loss: 0.676323   Test Accuracy: 67.490\n",
      "Epoch: 33 \tTraining Loss: 0.666102   Test Accuracy: 67.140\n",
      "Epoch: 34 \tTraining Loss: 0.669618   Test Accuracy: 66.960\n",
      "Epoch: 35 \tTraining Loss: 0.666267   Test Accuracy: 67.230\n",
      "Epoch: 36 \tTraining Loss: 0.656056   Test Accuracy: 67.110\n",
      "Epoch: 37 \tTraining Loss: 0.650942   Test Accuracy: 67.280\n",
      "Epoch: 38 \tTraining Loss: 0.645092   Test Accuracy: 66.830\n",
      "Epoch: 39 \tTraining Loss: 0.641339   Test Accuracy: 67.250\n",
      "Epoch: 40 \tTraining Loss: 0.638375   Test Accuracy: 67.410\n",
      "Epoch: 41 \tTraining Loss: 0.638125   Test Accuracy: 66.390\n",
      "Epoch: 42 \tTraining Loss: 0.633100   Test Accuracy: 66.360\n",
      "Epoch: 43 \tTraining Loss: 0.624598   Test Accuracy: 66.680\n",
      "Epoch: 44 \tTraining Loss: 0.624971   Test Accuracy: 66.730\n",
      "Epoch: 45 \tTraining Loss: 0.623405   Test Accuracy: 66.970\n",
      "Epoch: 46 \tTraining Loss: 0.615131   Test Accuracy: 65.820\n",
      "Epoch: 47 \tTraining Loss: 0.614814   Test Accuracy: 66.950\n",
      "Epoch: 48 \tTraining Loss: 0.613298   Test Accuracy: 67.230\n",
      "Epoch: 49 \tTraining Loss: 0.611257   Test Accuracy: 66.680\n",
      "Epoch: 50 \tTraining Loss: 0.604504   Test Accuracy: 67.610\n"
     ]
    }
   ],
   "source": [
    "lenet = LeNet().to(device)\n",
    "adam_lenet = optim.Adam(lenet.parameters(), lr=1e-3) #Adam\n",
    "lenet_adam, loss_adam, acc_adam = train_test(lenet, train_dl, test_dl, adam_lenet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6(A). TRAINING ALEXNET WITH SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.302524   Test Accuracy: 10.000\n",
      "Epoch: 2 \tTraining Loss: 2.302311   Test Accuracy: 10.000\n",
      "Epoch: 3 \tTraining Loss: 2.302094   Test Accuracy: 10.000\n",
      "Epoch: 4 \tTraining Loss: 2.301833   Test Accuracy: 10.000\n",
      "Epoch: 5 \tTraining Loss: 2.301485   Test Accuracy: 13.750\n",
      "Epoch: 6 \tTraining Loss: 2.300991   Test Accuracy: 21.320\n",
      "Epoch: 7 \tTraining Loss: 2.300223   Test Accuracy: 22.440\n",
      "Epoch: 8 \tTraining Loss: 2.298916   Test Accuracy: 21.490\n",
      "Epoch: 9 \tTraining Loss: 2.296408   Test Accuracy: 20.640\n",
      "Epoch: 10 \tTraining Loss: 2.290299   Test Accuracy: 19.630\n",
      "Epoch: 11 \tTraining Loss: 2.265548   Test Accuracy: 19.270\n",
      "Epoch: 12 \tTraining Loss: 2.136194   Test Accuracy: 25.890\n",
      "Epoch: 13 \tTraining Loss: 1.980355   Test Accuracy: 30.140\n",
      "Epoch: 14 \tTraining Loss: 1.891064   Test Accuracy: 33.780\n",
      "Epoch: 15 \tTraining Loss: 1.803016   Test Accuracy: 36.530\n",
      "Epoch: 16 \tTraining Loss: 1.713240   Test Accuracy: 36.030\n",
      "Epoch: 17 \tTraining Loss: 1.643893   Test Accuracy: 39.120\n",
      "Epoch: 18 \tTraining Loss: 1.597160   Test Accuracy: 41.650\n",
      "Epoch: 19 \tTraining Loss: 1.566260   Test Accuracy: 42.840\n",
      "Epoch: 20 \tTraining Loss: 1.540999   Test Accuracy: 42.890\n",
      "Epoch: 21 \tTraining Loss: 1.517232   Test Accuracy: 43.210\n",
      "Epoch: 22 \tTraining Loss: 1.494968   Test Accuracy: 43.880\n",
      "Epoch: 23 \tTraining Loss: 1.474419   Test Accuracy: 46.410\n",
      "Epoch: 24 \tTraining Loss: 1.452547   Test Accuracy: 47.430\n",
      "Epoch: 25 \tTraining Loss: 1.429901   Test Accuracy: 45.900\n",
      "Epoch: 26 \tTraining Loss: 1.402857   Test Accuracy: 49.450\n",
      "Epoch: 27 \tTraining Loss: 1.381063   Test Accuracy: 49.750\n",
      "Epoch: 28 \tTraining Loss: 1.355788   Test Accuracy: 49.820\n",
      "Epoch: 29 \tTraining Loss: 1.333101   Test Accuracy: 50.940\n",
      "Epoch: 30 \tTraining Loss: 1.309506   Test Accuracy: 52.640\n",
      "Epoch: 31 \tTraining Loss: 1.288718   Test Accuracy: 52.380\n",
      "Epoch: 32 \tTraining Loss: 1.267784   Test Accuracy: 53.820\n",
      "Epoch: 33 \tTraining Loss: 1.245323   Test Accuracy: 55.360\n",
      "Epoch: 34 \tTraining Loss: 1.227238   Test Accuracy: 53.580\n",
      "Epoch: 35 \tTraining Loss: 1.206976   Test Accuracy: 55.010\n",
      "Epoch: 36 \tTraining Loss: 1.184467   Test Accuracy: 57.970\n",
      "Epoch: 37 \tTraining Loss: 1.168434   Test Accuracy: 57.280\n",
      "Epoch: 38 \tTraining Loss: 1.151397   Test Accuracy: 58.170\n",
      "Epoch: 39 \tTraining Loss: 1.126381   Test Accuracy: 59.520\n",
      "Epoch: 40 \tTraining Loss: 1.110317   Test Accuracy: 60.200\n",
      "Epoch: 41 \tTraining Loss: 1.091830   Test Accuracy: 60.370\n",
      "Epoch: 42 \tTraining Loss: 1.073486   Test Accuracy: 61.500\n",
      "Epoch: 43 \tTraining Loss: 1.052784   Test Accuracy: 61.550\n",
      "Epoch: 44 \tTraining Loss: 1.034854   Test Accuracy: 63.750\n",
      "Epoch: 45 \tTraining Loss: 1.014322   Test Accuracy: 62.410\n",
      "Epoch: 46 \tTraining Loss: 0.997386   Test Accuracy: 62.980\n",
      "Epoch: 47 \tTraining Loss: 0.979007   Test Accuracy: 63.990\n",
      "Epoch: 48 \tTraining Loss: 0.962489   Test Accuracy: 62.710\n",
      "Epoch: 49 \tTraining Loss: 0.945176   Test Accuracy: 62.130\n",
      "Epoch: 50 \tTraining Loss: 0.926305   Test Accuracy: 61.760\n"
     ]
    }
   ],
   "source": [
    "alexnet = AlexNet().to(device)\n",
    "sgd_alexnet = optim.SGD(alexnet.parameters(), lr=1e-3) #SGD\n",
    "alexnet_sgd, a_loss_sgd, a_acc_sgd = train_test(alexnet, train_dl, test_dl, sgd_alexnet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6(B). TRAINING ALEXNET WITH SGD+MOMENTUM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.300221   Test Accuracy: 20.180\n",
      "Epoch: 2 \tTraining Loss: 1.909889   Test Accuracy: 40.000\n",
      "Epoch: 3 \tTraining Loss: 1.537041   Test Accuracy: 47.350\n",
      "Epoch: 4 \tTraining Loss: 1.381929   Test Accuracy: 49.520\n",
      "Epoch: 5 \tTraining Loss: 1.265703   Test Accuracy: 57.060\n",
      "Epoch: 6 \tTraining Loss: 1.164866   Test Accuracy: 58.980\n",
      "Epoch: 7 \tTraining Loss: 1.049995   Test Accuracy: 63.920\n",
      "Epoch: 8 \tTraining Loss: 0.945971   Test Accuracy: 66.410\n",
      "Epoch: 9 \tTraining Loss: 0.848259   Test Accuracy: 70.580\n",
      "Epoch: 10 \tTraining Loss: 0.760642   Test Accuracy: 72.470\n",
      "Epoch: 11 \tTraining Loss: 0.683422   Test Accuracy: 74.620\n",
      "Epoch: 12 \tTraining Loss: 0.619311   Test Accuracy: 76.590\n",
      "Epoch: 13 \tTraining Loss: 0.560506   Test Accuracy: 77.430\n",
      "Epoch: 14 \tTraining Loss: 0.505934   Test Accuracy: 78.090\n",
      "Epoch: 15 \tTraining Loss: 0.455036   Test Accuracy: 79.530\n",
      "Epoch: 16 \tTraining Loss: 0.400832   Test Accuracy: 80.260\n",
      "Epoch: 17 \tTraining Loss: 0.361593   Test Accuracy: 79.840\n",
      "Epoch: 18 \tTraining Loss: 0.308144   Test Accuracy: 80.580\n",
      "Epoch: 19 \tTraining Loss: 0.265205   Test Accuracy: 80.970\n",
      "Epoch: 20 \tTraining Loss: 0.223508   Test Accuracy: 81.150\n",
      "Epoch: 21 \tTraining Loss: 0.185550   Test Accuracy: 80.090\n",
      "Epoch: 22 \tTraining Loss: 0.146085   Test Accuracy: 81.440\n",
      "Epoch: 23 \tTraining Loss: 0.125761   Test Accuracy: 80.270\n",
      "Epoch: 24 \tTraining Loss: 0.101661   Test Accuracy: 81.860\n",
      "Epoch: 25 \tTraining Loss: 0.083558   Test Accuracy: 81.300\n",
      "Epoch: 26 \tTraining Loss: 0.073631   Test Accuracy: 82.220\n",
      "Epoch: 27 \tTraining Loss: 0.060522   Test Accuracy: 80.340\n",
      "Epoch: 28 \tTraining Loss: 0.052265   Test Accuracy: 81.780\n",
      "Epoch: 29 \tTraining Loss: 0.044113   Test Accuracy: 80.390\n",
      "Epoch: 30 \tTraining Loss: 0.037167   Test Accuracy: 81.980\n",
      "Epoch: 31 \tTraining Loss: 0.031355   Test Accuracy: 82.640\n",
      "Epoch: 32 \tTraining Loss: 0.030357   Test Accuracy: 81.700\n",
      "Epoch: 33 \tTraining Loss: 0.028993   Test Accuracy: 82.410\n",
      "Epoch: 34 \tTraining Loss: 0.021182   Test Accuracy: 81.780\n",
      "Epoch: 35 \tTraining Loss: 0.021697   Test Accuracy: 82.350\n",
      "Epoch: 36 \tTraining Loss: 0.015056   Test Accuracy: 81.850\n",
      "Epoch: 37 \tTraining Loss: 0.019793   Test Accuracy: 81.410\n",
      "Epoch: 38 \tTraining Loss: 0.016505   Test Accuracy: 81.610\n",
      "Epoch: 39 \tTraining Loss: 0.013595   Test Accuracy: 82.800\n",
      "Epoch: 40 \tTraining Loss: 0.013027   Test Accuracy: 82.880\n",
      "Epoch: 41 \tTraining Loss: 0.016293   Test Accuracy: 82.250\n",
      "Epoch: 42 \tTraining Loss: 0.006590   Test Accuracy: 81.850\n",
      "Epoch: 43 \tTraining Loss: 0.003253   Test Accuracy: 83.090\n",
      "Epoch: 44 \tTraining Loss: 0.002073   Test Accuracy: 83.040\n",
      "Epoch: 45 \tTraining Loss: 0.002264   Test Accuracy: 83.010\n",
      "Epoch: 46 \tTraining Loss: 0.001839   Test Accuracy: 83.370\n",
      "Epoch: 47 \tTraining Loss: 0.004209   Test Accuracy: 83.260\n",
      "Epoch: 48 \tTraining Loss: 0.017736   Test Accuracy: 82.330\n",
      "Epoch: 49 \tTraining Loss: 0.007740   Test Accuracy: 81.450\n",
      "Epoch: 50 \tTraining Loss: 0.005996   Test Accuracy: 82.750\n"
     ]
    }
   ],
   "source": [
    "alexnet = AlexNet().to(device)\n",
    "sgdm_alexnet = optim.SGD(alexnet.parameters(), lr=1e-3, momentum=0.9) #SGD+momentum\n",
    "alexnet_sgdm, a_loss_sgdm, a_acc_sgdm = train_test(alexnet, train_dl, test_dl, sgdm_alexnet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6(C). TRAINING ALEXNET WITH ADAGRAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.411487   Test Accuracy: 57.820\n",
      "Epoch: 2 \tTraining Loss: 1.005781   Test Accuracy: 66.860\n",
      "Epoch: 3 \tTraining Loss: 0.845419   Test Accuracy: 70.830\n",
      "Epoch: 4 \tTraining Loss: 0.746006   Test Accuracy: 74.100\n",
      "Epoch: 5 \tTraining Loss: 0.673508   Test Accuracy: 72.180\n",
      "Epoch: 6 \tTraining Loss: 0.614633   Test Accuracy: 76.340\n",
      "Epoch: 7 \tTraining Loss: 0.569588   Test Accuracy: 78.150\n",
      "Epoch: 8 \tTraining Loss: 0.528160   Test Accuracy: 78.090\n",
      "Epoch: 9 \tTraining Loss: 0.493159   Test Accuracy: 79.390\n",
      "Epoch: 10 \tTraining Loss: 0.462638   Test Accuracy: 79.500\n",
      "Epoch: 11 \tTraining Loss: 0.436277   Test Accuracy: 80.150\n",
      "Epoch: 12 \tTraining Loss: 0.410992   Test Accuracy: 80.290\n",
      "Epoch: 13 \tTraining Loss: 0.388640   Test Accuracy: 80.460\n",
      "Epoch: 14 \tTraining Loss: 0.364556   Test Accuracy: 81.630\n",
      "Epoch: 15 \tTraining Loss: 0.344643   Test Accuracy: 81.220\n",
      "Epoch: 16 \tTraining Loss: 0.323256   Test Accuracy: 82.440\n",
      "Epoch: 17 \tTraining Loss: 0.305549   Test Accuracy: 81.710\n",
      "Epoch: 18 \tTraining Loss: 0.287891   Test Accuracy: 82.520\n",
      "Epoch: 19 \tTraining Loss: 0.267876   Test Accuracy: 82.540\n",
      "Epoch: 20 \tTraining Loss: 0.251446   Test Accuracy: 82.770\n",
      "Epoch: 21 \tTraining Loss: 0.233673   Test Accuracy: 82.200\n",
      "Epoch: 22 \tTraining Loss: 0.219355   Test Accuracy: 82.760\n",
      "Epoch: 23 \tTraining Loss: 0.202792   Test Accuracy: 82.620\n",
      "Epoch: 24 \tTraining Loss: 0.187938   Test Accuracy: 81.970\n",
      "Epoch: 25 \tTraining Loss: 0.172827   Test Accuracy: 81.730\n",
      "Epoch: 26 \tTraining Loss: 0.160035   Test Accuracy: 82.030\n",
      "Epoch: 27 \tTraining Loss: 0.145756   Test Accuracy: 82.910\n",
      "Epoch: 28 \tTraining Loss: 0.131948   Test Accuracy: 82.150\n",
      "Epoch: 29 \tTraining Loss: 0.119291   Test Accuracy: 83.040\n",
      "Epoch: 30 \tTraining Loss: 0.107234   Test Accuracy: 82.270\n",
      "Epoch: 31 \tTraining Loss: 0.097027   Test Accuracy: 83.090\n",
      "Epoch: 32 \tTraining Loss: 0.086927   Test Accuracy: 82.370\n",
      "Epoch: 33 \tTraining Loss: 0.077528   Test Accuracy: 82.970\n",
      "Epoch: 34 \tTraining Loss: 0.069501   Test Accuracy: 82.720\n",
      "Epoch: 35 \tTraining Loss: 0.058863   Test Accuracy: 83.460\n",
      "Epoch: 36 \tTraining Loss: 0.051980   Test Accuracy: 83.100\n",
      "Epoch: 37 \tTraining Loss: 0.045336   Test Accuracy: 83.350\n",
      "Epoch: 38 \tTraining Loss: 0.039663   Test Accuracy: 83.070\n",
      "Epoch: 39 \tTraining Loss: 0.034049   Test Accuracy: 81.700\n",
      "Epoch: 40 \tTraining Loss: 0.029633   Test Accuracy: 83.070\n",
      "Epoch: 41 \tTraining Loss: 0.023738   Test Accuracy: 83.420\n",
      "Epoch: 42 \tTraining Loss: 0.020947   Test Accuracy: 83.380\n",
      "Epoch: 43 \tTraining Loss: 0.016914   Test Accuracy: 83.330\n",
      "Epoch: 44 \tTraining Loss: 0.014398   Test Accuracy: 83.550\n",
      "Epoch: 45 \tTraining Loss: 0.011391   Test Accuracy: 83.120\n",
      "Epoch: 46 \tTraining Loss: 0.010005   Test Accuracy: 83.090\n",
      "Epoch: 47 \tTraining Loss: 0.008435   Test Accuracy: 83.240\n",
      "Epoch: 48 \tTraining Loss: 0.007264   Test Accuracy: 82.910\n",
      "Epoch: 49 \tTraining Loss: 0.006313   Test Accuracy: 82.910\n",
      "Epoch: 50 \tTraining Loss: 0.005337   Test Accuracy: 82.790\n"
     ]
    }
   ],
   "source": [
    "alexnet = AlexNet().to(device)\n",
    "ag_alexnet = optim.Adagrad(alexnet.parameters(), lr=1e-3) #adagrad\n",
    "alexnet_ag, a_loss_ag, a_acc_ag = train_test(alexnet, train_dl, test_dl, ag_alexnet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6(D). TRAINING ALEXNET WITH RMSPROP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 781.802014   Test Accuracy: 42.070\n",
      "Epoch: 2 \tTraining Loss: 1.277968   Test Accuracy: 61.930\n",
      "Epoch: 3 \tTraining Loss: 0.939784   Test Accuracy: 71.400\n",
      "Epoch: 4 \tTraining Loss: 0.785330   Test Accuracy: 66.880\n",
      "Epoch: 5 \tTraining Loss: 0.701847   Test Accuracy: 72.790\n",
      "Epoch: 6 \tTraining Loss: 0.643998   Test Accuracy: 70.570\n",
      "Epoch: 7 \tTraining Loss: 0.597814   Test Accuracy: 71.500\n",
      "Epoch: 8 \tTraining Loss: 0.568861   Test Accuracy: 76.530\n",
      "Epoch: 9 \tTraining Loss: 0.535063   Test Accuracy: 77.620\n",
      "Epoch: 10 \tTraining Loss: 0.514097   Test Accuracy: 77.260\n",
      "Epoch: 11 \tTraining Loss: 0.509815   Test Accuracy: 78.210\n",
      "Epoch: 12 \tTraining Loss: 0.485198   Test Accuracy: 75.270\n",
      "Epoch: 13 \tTraining Loss: 0.545038   Test Accuracy: 78.370\n",
      "Epoch: 14 \tTraining Loss: 0.462339   Test Accuracy: 80.740\n",
      "Epoch: 15 \tTraining Loss: 0.451865   Test Accuracy: 77.530\n",
      "Epoch: 16 \tTraining Loss: 0.435248   Test Accuracy: 79.960\n",
      "Epoch: 17 \tTraining Loss: 0.430961   Test Accuracy: 79.890\n",
      "Epoch: 18 \tTraining Loss: 0.422618   Test Accuracy: 81.580\n",
      "Epoch: 19 \tTraining Loss: 1.401781   Test Accuracy: 79.690\n",
      "Epoch: 20 \tTraining Loss: 0.435241   Test Accuracy: 77.210\n",
      "Epoch: 21 \tTraining Loss: 0.412566   Test Accuracy: 80.200\n",
      "Epoch: 22 \tTraining Loss: 0.428478   Test Accuracy: 76.460\n",
      "Epoch: 23 \tTraining Loss: 0.428852   Test Accuracy: 76.160\n",
      "Epoch: 24 \tTraining Loss: 0.477719   Test Accuracy: 79.400\n",
      "Epoch: 25 \tTraining Loss: 1674.576512   Test Accuracy: 73.250\n",
      "Epoch: 26 \tTraining Loss: 1.626162   Test Accuracy: 79.550\n",
      "Epoch: 27 \tTraining Loss: 0.536298   Test Accuracy: 78.410\n",
      "Epoch: 28 \tTraining Loss: 29.037810   Test Accuracy: 77.210\n",
      "Epoch: 29 \tTraining Loss: 0.412515   Test Accuracy: 73.300\n",
      "Epoch: 30 \tTraining Loss: 16.239384   Test Accuracy: 80.090\n",
      "Epoch: 31 \tTraining Loss: 0.498563   Test Accuracy: 81.040\n",
      "Epoch: 32 \tTraining Loss: 0.440250   Test Accuracy: 80.920\n",
      "Epoch: 33 \tTraining Loss: 14.900761   Test Accuracy: 80.170\n",
      "Epoch: 34 \tTraining Loss: 0.445784   Test Accuracy: 81.760\n",
      "Epoch: 35 \tTraining Loss: 0.469302   Test Accuracy: 74.210\n",
      "Epoch: 36 \tTraining Loss: 0.790939   Test Accuracy: 63.060\n",
      "Epoch: 37 \tTraining Loss: 0.613810   Test Accuracy: 81.910\n",
      "Epoch: 38 \tTraining Loss: 0.495075   Test Accuracy: 81.650\n",
      "Epoch: 39 \tTraining Loss: 0.468002   Test Accuracy: 75.630\n",
      "Epoch: 40 \tTraining Loss: 0.459753   Test Accuracy: 72.970\n",
      "Epoch: 41 \tTraining Loss: 0.458748   Test Accuracy: 73.700\n",
      "Epoch: 42 \tTraining Loss: 0.600665   Test Accuracy: 76.080\n",
      "Epoch: 43 \tTraining Loss: 2.494797   Test Accuracy: 77.000\n",
      "Epoch: 44 \tTraining Loss: 0.472574   Test Accuracy: 79.720\n",
      "Epoch: 45 \tTraining Loss: 0.630499   Test Accuracy: 78.640\n",
      "Epoch: 46 \tTraining Loss: 0.601197   Test Accuracy: 74.340\n",
      "Epoch: 47 \tTraining Loss: 1.316696   Test Accuracy: 79.110\n",
      "Epoch: 48 \tTraining Loss: 5.166928   Test Accuracy: 48.960\n",
      "Epoch: 49 \tTraining Loss: 6.041044   Test Accuracy: 22.540\n",
      "Epoch: 50 \tTraining Loss: 1.173774   Test Accuracy: 76.300\n"
     ]
    }
   ],
   "source": [
    "alexnet = AlexNet().to(device)\n",
    "rms_alexnet = optim.RMSprop(alexnet.parameters(), lr=1e-3) #rmsprop\n",
    "alexnet_rms, a_loss_rms, a_acc_rms = train_test(alexnet, train_dl, test_dl, rms_alexnet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6(E). TRAINING ALEXNET WITH ADAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.559537   Test Accuracy: 56.710\n",
      "Epoch: 2 \tTraining Loss: 1.100123   Test Accuracy: 63.480\n",
      "Epoch: 3 \tTraining Loss: 0.926556   Test Accuracy: 68.440\n",
      "Epoch: 4 \tTraining Loss: 0.810955   Test Accuracy: 69.820\n",
      "Epoch: 5 \tTraining Loss: 0.730435   Test Accuracy: 72.450\n",
      "Epoch: 6 \tTraining Loss: 0.659948   Test Accuracy: 72.850\n",
      "Epoch: 7 \tTraining Loss: 0.608346   Test Accuracy: 74.960\n",
      "Epoch: 8 \tTraining Loss: 0.561820   Test Accuracy: 75.960\n",
      "Epoch: 9 \tTraining Loss: 0.516962   Test Accuracy: 76.090\n",
      "Epoch: 10 \tTraining Loss: 0.477572   Test Accuracy: 76.590\n",
      "Epoch: 11 \tTraining Loss: 0.452566   Test Accuracy: 76.000\n",
      "Epoch: 12 \tTraining Loss: 0.415723   Test Accuracy: 76.860\n",
      "Epoch: 13 \tTraining Loss: 0.391954   Test Accuracy: 76.880\n",
      "Epoch: 14 \tTraining Loss: 0.361552   Test Accuracy: 76.010\n",
      "Epoch: 15 \tTraining Loss: 0.343404   Test Accuracy: 75.570\n",
      "Epoch: 16 \tTraining Loss: 0.319857   Test Accuracy: 77.460\n",
      "Epoch: 17 \tTraining Loss: 0.300923   Test Accuracy: 76.880\n",
      "Epoch: 18 \tTraining Loss: 0.290968   Test Accuracy: 75.280\n",
      "Epoch: 19 \tTraining Loss: 0.266123   Test Accuracy: 75.950\n",
      "Epoch: 20 \tTraining Loss: 0.266410   Test Accuracy: 76.330\n",
      "Epoch: 21 \tTraining Loss: 0.245056   Test Accuracy: 77.090\n",
      "Epoch: 22 \tTraining Loss: 0.238356   Test Accuracy: 76.220\n",
      "Epoch: 23 \tTraining Loss: 0.218799   Test Accuracy: 75.700\n",
      "Epoch: 24 \tTraining Loss: 0.220766   Test Accuracy: 76.320\n",
      "Epoch: 25 \tTraining Loss: 0.209057   Test Accuracy: 77.060\n",
      "Epoch: 26 \tTraining Loss: 0.206052   Test Accuracy: 76.740\n",
      "Epoch: 27 \tTraining Loss: 0.194406   Test Accuracy: 77.220\n",
      "Epoch: 28 \tTraining Loss: 0.184531   Test Accuracy: 76.030\n",
      "Epoch: 29 \tTraining Loss: 0.186743   Test Accuracy: 76.420\n",
      "Epoch: 30 \tTraining Loss: 0.173779   Test Accuracy: 77.190\n",
      "Epoch: 31 \tTraining Loss: 0.168849   Test Accuracy: 76.520\n",
      "Epoch: 32 \tTraining Loss: 0.166425   Test Accuracy: 76.450\n",
      "Epoch: 33 \tTraining Loss: 0.154473   Test Accuracy: 76.740\n",
      "Epoch: 34 \tTraining Loss: 0.204744   Test Accuracy: 76.990\n",
      "Epoch: 35 \tTraining Loss: 0.150212   Test Accuracy: 77.440\n",
      "Epoch: 36 \tTraining Loss: 0.148026   Test Accuracy: 76.520\n",
      "Epoch: 37 \tTraining Loss: 0.134015   Test Accuracy: 76.910\n",
      "Epoch: 38 \tTraining Loss: 0.141375   Test Accuracy: 77.340\n",
      "Epoch: 39 \tTraining Loss: 0.155496   Test Accuracy: 76.050\n",
      "Epoch: 40 \tTraining Loss: 0.132760   Test Accuracy: 77.430\n",
      "Epoch: 41 \tTraining Loss: 0.140132   Test Accuracy: 75.600\n",
      "Epoch: 42 \tTraining Loss: 0.128189   Test Accuracy: 76.320\n",
      "Epoch: 43 \tTraining Loss: 0.127866   Test Accuracy: 77.110\n",
      "Epoch: 44 \tTraining Loss: 0.134860   Test Accuracy: 76.540\n",
      "Epoch: 45 \tTraining Loss: 0.122606   Test Accuracy: 76.560\n",
      "Epoch: 46 \tTraining Loss: 0.117773   Test Accuracy: 75.470\n",
      "Epoch: 47 \tTraining Loss: 0.127786   Test Accuracy: 76.900\n",
      "Epoch: 48 \tTraining Loss: 0.102996   Test Accuracy: 77.010\n",
      "Epoch: 49 \tTraining Loss: 0.123450   Test Accuracy: 76.840\n",
      "Epoch: 50 \tTraining Loss: 0.115509   Test Accuracy: 76.110\n"
     ]
    }
   ],
   "source": [
    "alexnet = AlexNet().to(device)\n",
    "adam_alexnet = optim.Adam(alexnet.parameters(), lr=1e-3) #Adam\n",
    "alexnet_adam, a_loss_adam, a_acc_adam = train_test(alexnet, train_dl, test_dl, adam_alexnet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.702230   Test Accuracy: 44.880\n",
      "Epoch: 2 \tTraining Loss: 1.410409   Test Accuracy: 49.030\n",
      "Epoch: 3 \tTraining Loss: 1.279573   Test Accuracy: 56.840\n",
      "Epoch: 4 \tTraining Loss: 1.174875   Test Accuracy: 59.250\n",
      "Epoch: 5 \tTraining Loss: 1.087011   Test Accuracy: 62.310\n",
      "Epoch: 6 \tTraining Loss: 1.008962   Test Accuracy: 64.220\n",
      "Epoch: 7 \tTraining Loss: 0.939828   Test Accuracy: 66.790\n",
      "Epoch: 8 \tTraining Loss: 0.881587   Test Accuracy: 68.470\n",
      "Epoch: 9 \tTraining Loss: 0.832144   Test Accuracy: 69.020\n",
      "Epoch: 10 \tTraining Loss: 0.779176   Test Accuracy: 71.620\n",
      "Epoch: 11 \tTraining Loss: 0.737359   Test Accuracy: 72.670\n",
      "Epoch: 12 \tTraining Loss: 0.694408   Test Accuracy: 72.890\n",
      "Epoch: 13 \tTraining Loss: 0.652612   Test Accuracy: 74.220\n",
      "Epoch: 14 \tTraining Loss: 0.614143   Test Accuracy: 75.480\n",
      "Epoch: 15 \tTraining Loss: 0.577868   Test Accuracy: 76.460\n",
      "Epoch: 16 \tTraining Loss: 0.536834   Test Accuracy: 76.000\n",
      "Epoch: 17 \tTraining Loss: 0.502251   Test Accuracy: 77.260\n",
      "Epoch: 18 \tTraining Loss: 0.465969   Test Accuracy: 77.860\n",
      "Epoch: 19 \tTraining Loss: 0.431217   Test Accuracy: 76.950\n",
      "Epoch: 20 \tTraining Loss: 0.399947   Test Accuracy: 77.430\n",
      "Epoch: 21 \tTraining Loss: 0.363388   Test Accuracy: 78.790\n",
      "Epoch: 22 \tTraining Loss: 0.331914   Test Accuracy: 78.490\n",
      "Epoch: 23 \tTraining Loss: 0.298665   Test Accuracy: 79.280\n",
      "Epoch: 24 \tTraining Loss: 0.268582   Test Accuracy: 79.380\n",
      "Epoch: 25 \tTraining Loss: 0.237867   Test Accuracy: 79.190\n",
      "Epoch: 26 \tTraining Loss: 0.207231   Test Accuracy: 80.010\n",
      "Epoch: 27 \tTraining Loss: 0.178632   Test Accuracy: 79.820\n",
      "Epoch: 28 \tTraining Loss: 0.155547   Test Accuracy: 79.100\n",
      "Epoch: 29 \tTraining Loss: 0.133852   Test Accuracy: 78.320\n",
      "Epoch: 30 \tTraining Loss: 0.116575   Test Accuracy: 79.510\n",
      "Epoch: 31 \tTraining Loss: 0.099252   Test Accuracy: 79.930\n",
      "Epoch: 32 \tTraining Loss: 0.087875   Test Accuracy: 78.780\n",
      "Epoch: 33 \tTraining Loss: 0.069970   Test Accuracy: 79.560\n",
      "Epoch: 34 \tTraining Loss: 0.065710   Test Accuracy: 80.040\n",
      "Epoch: 35 \tTraining Loss: 0.057748   Test Accuracy: 79.830\n",
      "Epoch: 36 \tTraining Loss: 0.055224   Test Accuracy: 79.980\n",
      "Epoch: 37 \tTraining Loss: 0.046337   Test Accuracy: 79.740\n",
      "Epoch: 38 \tTraining Loss: 0.045020   Test Accuracy: 80.160\n",
      "Epoch: 39 \tTraining Loss: 0.043445   Test Accuracy: 79.260\n",
      "Epoch: 40 \tTraining Loss: 0.035822   Test Accuracy: 80.440\n",
      "Epoch: 41 \tTraining Loss: 0.038656   Test Accuracy: 80.300\n",
      "Epoch: 42 \tTraining Loss: 0.035572   Test Accuracy: 80.050\n",
      "Epoch: 43 \tTraining Loss: 0.032268   Test Accuracy: 80.350\n",
      "Epoch: 44 \tTraining Loss: 0.031585   Test Accuracy: 80.190\n",
      "Epoch: 45 \tTraining Loss: 0.027840   Test Accuracy: 79.380\n",
      "Epoch: 46 \tTraining Loss: 0.029762   Test Accuracy: 80.710\n",
      "Epoch: 47 \tTraining Loss: 0.024746   Test Accuracy: 79.730\n",
      "Epoch: 48 \tTraining Loss: 0.027081   Test Accuracy: 79.970\n",
      "Epoch: 49 \tTraining Loss: 0.026202   Test Accuracy: 79.470\n",
      "Epoch: 50 \tTraining Loss: 0.023688   Test Accuracy: 79.640\n"
     ]
    }
   ],
   "source": [
    "alexnet = AlexNet().to(device)\n",
    "adam_alexnet = optim.Adam(alexnet.parameters(), lr=1e-5) #Adam\n",
    "alexnet_adam, a_loss_adam, a_acc_adam = train_test(alexnet, train_dl, test_dl, adam_alexnet, n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLOTTING THE RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(a_loss_sgd, label = \"sgd\")\n",
    "plt.plot(a_loss_sgdm, label = \"sgdm\")\n",
    "plt.plot(a_loss_ag, label = \"adagrad\")\n",
    "plt.plot(a_loss_rms, label = \"rmsprop\")\n",
    "plt.plot(a_loss_adam, label = \"adam\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"AlexNet train loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a_acc_sgd, label = \"sgd\")\n",
    "plt.plot(a_acc_sgdm, label = \"sgdm\")\n",
    "plt.plot(a_acc_ag, label = \"adagrad\")\n",
    "plt.plot(a_acc_rms, label = \"rmsprop\")\n",
    "plt.plot(a_acc_adam, label = \"adam\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"AlexNet test accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. FURTHER EXPERIMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77ea62aefc826ad60bd92e68cbd3f3114bc6d7e920db1ef6da6e6811c5006655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
